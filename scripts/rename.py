import os
import json
import argparse
import shutil
import re


def load_tokenizer_config(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_tokenizer_config(config_path, data):
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def extract_inject_folder(auto_map_string):
    """
    è¾“å…¥: "mix_Rw3Q5g/tokenizer.MixTokenizer"
    è¿”å›: "mix_Rw3Q5g"
    """
    if not isinstance(auto_map_string, str):
        return None
    return auto_map_string.split("/")[0]


def update_tokenizer_py(py_path, new_inject_name):
    """æ›¿æ¢ tokenizer.py ä¸­ dir_name = "..."""
    with open(py_path, "r", encoding="utf-8") as f:
        content = f.read()

    new_content = re.sub(
        r'dir_name\s*=\s*"(.*?)"',
        f'dir_name = "{new_inject_name}"',
        content
    )

    with open(py_path, "w", encoding="utf-8") as f:
        f.write(new_content)


def main(root_dir, new_inject_name=None):
    tokenizer_config_path = os.path.join(root_dir, "tokenizer_config.json")
    if not os.path.exists(tokenizer_config_path):
        print(f"âŒ æ‰¾ä¸åˆ° tokenizer_config.json: {tokenizer_config_path}")
        return

    data = load_tokenizer_config(tokenizer_config_path)

    # è¯»å– auto_map
    try:
        auto_tok_entry = data["auto_map"]["AutoTokenizer"][0]
    except Exception:
        print("âŒ tokenizer_config.json ä¸­ç¼ºå°‘ auto_map.AutoTokenizer ä¿¡æ¯ï¼")
        return

    old_inject = extract_inject_folder(auto_tok_entry)

    if old_inject:
        print(f"å½“å‰ Inject æ–‡ä»¶å¤¹åï¼š{old_inject}")
    else:
        print("âŒ æ— æ³•è§£æ Inject æ–‡ä»¶å¤¹åï¼Œæ£€æŸ¥ tokenizer_config.json")
        return

    if not new_inject_name:
        print("æœªæŒ‡å®šæ–°çš„ Inject åç§°ï¼šä»…æŠ¥å‘Šï¼Œä¸ä¿®æ”¹ã€‚")
        return

    # --------------- ä¿®æ”¹ tokenizer_config.json ---------------
    new_auto_map_str = f"{new_inject_name}/tokenizer.MixTokenizer"
    data["auto_map"]["AutoTokenizer"][0] = new_auto_map_str
    save_tokenizer_config(tokenizer_config_path, data)
    print(f"âœ” å·²æ›´æ–° tokenizer_config.json: {new_auto_map_str}")

    # --------------- ä¿®æ”¹ tokenizer.py -------------------------
    old_dir = os.path.join(root_dir, old_inject)
    if not os.path.exists(old_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨ï¼š{old_dir}")
        return

    tokenizer_py_path = os.path.join(old_dir, "tokenizer.py")
    if not os.path.exists(tokenizer_py_path):
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{tokenizer_py_path}")
        return

    update_tokenizer_py(tokenizer_py_path, new_inject_name)
    print(f"âœ” å·²ä¿®æ”¹ {tokenizer_py_path} å†… dir_name å­—æ®µ")

    # --------------- é‡å‘½åæ–‡ä»¶å¤¹ -----------------------------
    new_dir = os.path.join(root_dir, new_inject_name)
    shutil.move(old_dir, new_dir)
    print(f"âœ” å·²å°†ç›®å½• {old_inject} é‡å‘½åä¸º {new_inject_name}")

    print("ğŸ‰ å…¨éƒ¨æ­¥éª¤å®Œæˆï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Inject Folder Renamer")
    parser.add_argument("dir", type=str, help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--new", type=str, default=None,
                        help="æ–°çš„ Inject æ–‡ä»¶å¤¹åå­— (å¯é€‰)")
    args = parser.parse_args()

    main(args.dir, args.new)
